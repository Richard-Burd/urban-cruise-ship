import SolutionDropdown from '/components/SolutionDropdown.js'

export const problem = "Risk From Artificial Intelligence";
export const solution = "AI Safety Policy - World";

## Description
We recommend investing in ongoing AI risk assessment and global AI safety policy.


## Background
Artificial Intelligence (AI) development has accelerated rapidly in recent years. It is possible that AI machines will outperform humans in many tasks in the next few decades[^”1”]. This could bring many benefits however there could be unforeseen risks. Many AI experts think that there is a chance that AI will lead to negative outcomes, possibly even human extinction[^”1”]. 

Safety measures have not kept pace evenly across the globe. Currently, different regions are approaching AI governance in fragmented ways:  
European Union: Implemented the [EU AI Act (2024)](https://artificialintelligenceact.eu/), one of the most comprehensive regulatory frameworks to date, emphasizing risk categories (unacceptable, high, limited, minimal) and mandatory oversight.  
United States: Relies on a patchwork of voluntary guidelines such as [NIST AI Risk Management Framework]( https://www.nist.gov/itl/ai-risk-management-framework) and sector-specific regulations. Federal action is still largely advisory rather than binding.  
China: Enforces strict rules on generative AI output, with an emphasis on censorship, algorithm registration, and government oversight, though less focus is placed on long-term existential risks.  
Global: Initiatives like the [OECD AI Principles](https://www.oecd.org/en/topics/sub-issues/ai-principles.html), the [UN’s AI Advisory Body](https://www.un.org/en/ai-advisory-body), and the [G7 Hiroshima AI Process]( https://www.soumu.go.jp/hiroshimaaiprocess/en/index.html) push for shared ethical standards, but binding enforcement mechanisms remain limited.

There are many dangers of not having continual risk assessments and safety guidelines, some of which are not readily obvious. AI systems could scale disinformation or harmful bias loops thus amplifying misinformation[^”5”]. Ai used in healthcare, infrastructure, or defense systems could malfunction or be misused without robust testing and safety measures[^”3”]. If AI creates rapid job displacement there could be economic destabilization unless mitigation strategies are put into place. If any or multiple of these risks happen there could be public backlash or unforeseen outcomes.

Ongoing risks assessments for AI safety would ensure accountability for those producing AI.  Policies should be developed that build public trust by promoting the ethical use of AI for the benefit of society[^”4”]. Setting clear boundaries allows for responsible innovation. Setting global AI safety principals would provide stability and reduce the competitive “arms race” for this technology. Early detection protocols would allow for course correction to mitigate unintended consequences.

## Calculations
It is estimated that there are only around 400 people worldwide working on this issue[^”1”].

Around $50 million was spent on reducing catastrophic risks from AI in 2020, while billions were spent advancing AI capabilities[^”1”].

## Implementation
Third-party safety audits and stress tests of AI models should be required, Safety protocols will need continual risk assessment cycles, testing AI systems periodically not just prior to deployment. AI training data as well as model capabilities and limitations should be disclosed allowing for transparency including [explainable AI]( https://www.urbancruiseship.org/habitat/wellbeing/health_socio). Policy frameworks will need to be dynamic and able to adapt to technological advances[^”2”]. Standards should be aligned across borders to promote international collaboration and prevent regulatory arbitrage. There will need to be clear consequences for non-compliance, ranging from fines to suspension of AI deployment.

Competing regulatory frameworks may undermine effectiveness, meaning a global standard and regulatory system is really a key component. Compliance may be difficult to monitor and enforce, especially as technological capabilities may develop faster than policies. Some nations may prioritize domination over cooperation and some companies may value profits and innovation over safety and regulation. If guidelines exist but are poorly enforced that may lead to a false sense of security with the public assuming AI systems are safer than they are.

##### Lead researcher: Lee Nelson - Last updated: October 1, 2025

[^”1”]: Hilton, B. [“Preventing an AI-related Catastrophe”](https://80000hours.org/problem-profiles/artificial-intelligence/) 80,000 Hours, August 2022

[^”2”]: Janssen, M. [“Responsible governance of generative AI: conceptualizing GenAI as complex adaptive systems”]( https://doi.org/10.1093/polsoc/puae040) Policy and Society, January 2025

[^”3”]: Szadeczky, T. et al. [“Risk, regulation, and governance: evaluating artificial intelligence across diverse application scenarios.”]( https://doi.org/10.1057/s41284-025-00495-z) Security Journal, June 2025

[^”4”]: Agarwal, A. and Nene, M. [“A five-layer framework for AI governance: integrating regulation, standards, and certification”]( https://doi.org/10.48550/arXiv.2509.11332) Transforming Government: People, Process and Policy, September 2025.

[^”5”]: Glickman, M. and Sharot, T. [“How human-AI feedback loops alter human perceptual, emotional and social judgements”]( https://doi.org/10.1038/s41562-024-02077-2) Pub Med, December 2024.



export default ({ children }) => 
  <SolutionDropdown 
    problem={problem} 
    solution={solution}
  >
    {children}
  </SolutionDropdown>